link for db account from PC - https://dbc-6b1195e8-d047.cloud.databricks.com/editor/notebooks/422501899801386?o=2920981618425947#command/8252519641109742




Full refresh will truncate and recompute ALL tables in this pipeline from scratch. This can lead to data loss for non-idempotent sources. Are you sure you want to full refresh?
in cdc, key specified column should be unique, else only one row gets selected and other record goes missing

emp_name	course_id	course_completed	order_id	order_ts	age	_rescued_data
null	1	Record_deleted	null	2025-09-04 10:31:00	null	{"age":null,"_file_path":"/Volumes/workspace/default/course2/sample2.parquet"}
rohit	2	Y	34	2025-09-04 10:31:00	null	{"age":24.0,"_file_path":"/Volumes/workspace/default/course2/sample2.parquet"}

rescued data helps to run code even if some records are corrupt, just marks them as corrupt due to whatever issues.



## expectations
-- refresh > keeps ddl, updates data, keeps checkpoint
-- replace > if ddl to be updated of source, clears old checkpoint, reprocess data from start

--volume is used as a var defined in config of pipeline settings
--when defining materialised view, don't use from stream even if it's a streaming table

create or replace streaming table workspace.default.stream_bronze1
select * 
from stream read_files(
  --'/Volumes/workspace/default/course2',
  "${volume}",
  format => 'parquet');

  create or replace streaming table workspace.default.stream_silver_expectations1
  (
    constraint valid_name expect (emp_name in ('atul','roshan','rohit')),
    constraint valid_age expect (age > 23) on violation drop row
    --constraint valid_order_id expect (order_id is not null) on violation fail update
  )
  as
  select *,
  date_format(
         from_utc_timestamp(current_timestamp(), 'Asia/Kolkata'),
         'yyyy-MM-dd HH:mm:ss'
       ) AS processing_time
  --,_metadata.file_name as source_file
  from stream stream_bronze1;

  create or refresh materialized view mv_gold_agg
  select emp_name, count(course_id) as total_courses_completed
  from stream_silver_expectations1
  where course_completed = 'Y'
  group by emp_name;

## cdc
create or replace streaming table stream_silver_cdc
comment 'scd type 2 historical data';

apply changes into stream_silver_cdc
 from stream stream_bronze1
 inserts
 keys(course_id)
 apply as delete when course_completed = 'Record_deleted'
 sequence by order_ts
 columns * except (_rescued_data)
 stored as scd type 2;

-- _END_AT being not null means record is inactive and deleted
create or refresh materialized view mv_latest_records
as 
select 
emp_name,
course_id,
order_id,
__START_AT,
__END_AT,
row_number() over (partition by course_id order by __START_AT desc) as latest_record
from stream_silver_cdc
qualify latest_record = 1;
