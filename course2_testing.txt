Full refresh will truncate and recompute ALL tables in this pipeline from scratch. This can lead to data loss for non-idempotent sources. Are you sure you want to full refresh?
in cdc, key specified column should be unique, else only one row gets selected and other record goes missing

emp_name	course_id	course_completed	order_id	order_ts	age	_rescued_data
null	1	Record_deleted	null	2025-09-04 10:31:00	null	{"age":null,"_file_path":"/Volumes/workspace/default/course2/sample2.parquet"}
rohit	2	Y	34	2025-09-04 10:31:00	null	{"age":24.0,"_file_path":"/Volumes/workspace/default/course2/sample2.parquet"}

rescued data helps to run code even if some records are corrupt, just marks them as corrupt due to whatever issues.



## insert data in volume



import json
from datetime import datetime
import random
import pandas as pd

data = [
    {
        "emp_name": "himanshu",
        "course_id": 1,
        "course_completed": "Y",
        "order_id": 11,
        "age": 24
    },
    {
        "emp_name": "atul",
        "course_id": 1,
        "course_completed": "N",
        "order_id": 21,
        "age": 23
    },
    {
        "emp_name": "rohit",
        "course_id": 2,
        "course_completed": "Y",
        "order_id": 31,
        "age": 24
    },
    {
        "emp_name": "roshan",
        "course_id": 3,
        "course_completed": "N",
        "order_id": None,
        "age": 26
    },
    {
        "emp_name": "swap",
        "course_id": 4,
        "course_completed": "Y",
        "order_id": 51,
        "age": 24
    }
]

# Convert the data to a pandas DataFrame
df = pd.DataFrame(data)

# Save the DataFrame as a Parquet file in the specified volume
df.to_parquet("/Volumes/workspace/default/course2/sample1.parquet", index=False)

## copy a file in volume
  dbutils.fs.cp("/Volumes/workspace/default/course2/sample1.parquet", "/Volumes/workspace/default/course2/sample1_copy.parquet")

## read file from spark df, issue with integer type
  from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, LongType

# Define the schema
schema = StructType([
    StructField("emp_name", StringType(), True),
    StructField("course_id", LongType(), True),
    StructField("course_completed", StringType(), True),
    StructField("order_id", DoubleType(), True),
    StructField("age", LongType(), True)
])

# Read the JSON file with the specified schema
df = spark.read.format("parquet").schema(schema).load("/Volumes/workspace/default/course2/sample1.parquet")
display(df)



## expectations

  
-- refresh > keeps ddl, updates data, keeps checkpoint
-- replace > if ddl to be updated of source, clears old checkpoint, reprocess data from start

--volume is used as a var defined in config of pipeline settings
--when defining materialised view, don't use from stream even if it's a streaming table

create or replace streaming table workspace.default.stream_bronze1
select * 
from stream read_files(
  --'/Volumes/workspace/default/course2',
  "${volume}",
  format => 'parquet');

  create or replace streaming table workspace.default.stream_silver_expectations1
  (
    constraint valid_name expect (emp_name in ('atul','roshan','rohit')),
    constraint valid_age expect (age > 23) on violation drop row
    --constraint valid_order_id expect (order_id is not null) on violation fail update
  )
  as
  select *,
  current_timestamp() as processing_time
  --,_metadata.file_name as source_file
  from stream stream_bronze1;

  create or refresh materialized view mv_gold_agg
  select emp_name, count(course_id) as total_courses_completed
  from stream_silver_expectations1
  where course_completed = 'Y'
  group by emp_name;


## cdc

create or replace streaming table stream_silver_cdc
comment 'scd type 2 historical data';

apply changes into stream_silver_cdc
 from stream stream_bronze1
 inserts
 keys(course_id)
 apply as delete when course_completed = 'Record_deleted'
 sequence by order_ts
 columns * except (_rescued_data)
 stored as scd type 2;

-- _END_AT being not null means record is inactive and deleted
create or refresh materialized view mv_latest_records
as 
select 
emp_name,
course_id,
order_id,
__START_AT,
__END_AT,
row_number() over (partition by course_id order by __START_AT desc) as latest_record
from stream_silver_cdc
qualify latest_record = 1;
